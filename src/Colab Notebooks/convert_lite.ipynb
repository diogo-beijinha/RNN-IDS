{"cells":[{"cell_type":"markdown","metadata":{"id":"yHgttz4ZhkbV"},"source":["# Convert Models to Lite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wmBlQqdhXvs","outputId":"2a07d5c0-0bb6-406c-8613-37b07622eceb","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import tensorflow as tf\n","import sys\n","import os\n","\n","def solo_convert():\n","  # Import necessary files from GDrive\n","  sys.path.append('/content/drive/My Drive/Colab Notebooks/')\n","  !cp -r \"/content/drive/My Drive/Colab Notebooks/tf_models/\" '/content/'\n","  !cp -r \"/content/drive/My Drive/Colab Notebooks/quant_model/\" '/content/'\n","  model_path = \"/content/tf_models/\"\n","  quant_model_path = \"/content/quant_model/\"\n","  lite_model_path = \"/content/lite_models/\"\n","  quant_lite_model_path = \"/content/quant_lite_model/\"\n","\n","  # Convert Normal Model\n","  if os.path.exists(model_path):\n","    converter = tf.lite.TFLiteConverter.from_saved_model(model_path) # Create Tensorflow Converter Instance with the Model Path\n","    converter.optimizations = [tf.lite.Optimize.DEFAULT] # Allow Quantized Models\n","    converter.target_spec.supported_ops = [\n","        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n","        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n","    ]\n","    tflite_model = converter.convert() # Convert Model to Lite Model\n","\n","    # Save the model\n","    if not os.path.exists(lite_model_path):\n","                os.makedirs(lite_model_path)\n","    with open(\"/content/lite_models/model.tflite\", 'wb') as f:\n","        f.write(tflite_model)\n","\n","    print(\"\\n\\nConverted to TFLite!\")\n","\n","    !cp -r '/content/lite_models/' \"/content/drive/My Drive/Colab Notebooks/\" # Export model to GDrive\n","    print(\"Lite model exported to Drive Folder\")\n","  \n","\n","  # Convert Quantized Model to Lite Quantized Model\n","  if os.path.exists(quant_model_path):  \n","\n","    converter = tf.lite.TFLiteConverter.from_saved_model(quant_model_path) # Create Tensorflow Converter Instance with the Quantized Model Path\n","    converter.optimizations = [tf.lite.Optimize.DEFAULT] # Allow Quantized Models\n","    converter.target_spec.supported_ops = [\n","        tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n","        tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n","    ]\n","    quant_tflite_model = converter.convert() # Convert Model to Lite Model\n","\n","    # Save the model\n","    if not os.path.exists(quant_lite_model_path):\n","                os.makedirs(quant_lite_model_path)\n","    with open(\"/content/quant_lite_model/quant_model.tflite\", 'wb') as f:\n","        f.write(quant_tflite_model)\n","\n","    print(\"\\n\\nConverted to Quantized TFLite!\")\n","\n","    !cp -r '/content/quant_lite_model/' \"/content/drive/My Drive/Colab Notebooks/\" # Export Quantized Lite Model to GDrive\n","    print(\"Quantized Lite model exported to Drive Folder\")\n","\n","solo_convert()"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSadKNOO4mFJtduhhpgHlz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}